{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e784806c-2ba4-435c-87fd-744cdf5cf5bc",
   "metadata": {
    "id": "e784806c-2ba4-435c-87fd-744cdf5cf5bc"
   },
   "source": [
    "# 04 Creation of train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c2491-e632-44ac-bdef-f7537d54d8f8",
   "metadata": {},
   "source": [
    "In this notebook we will define a function to split the book into 5 book bundles, and we will create the test set from one and the four folds of the training set from the four others.\n",
    "\n",
    "We have to keep in mind two aspect.\n",
    "We would like to represent each author equally in both the train and test sets.\n",
    "Futhermore, to make our task more realistic, as already discussed and as explained below, the test set should come from different books/book groups than the train set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac33a6-9265-4ac5-9842-12c27bf05154",
   "metadata": {
    "id": "fcac33a6-9265-4ac5-9842-12c27bf05154",
    "tags": []
   },
   "source": [
    "## Selecting test/training/validation books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde3275-6327-489d-b3c4-e7bafd1a9f83",
   "metadata": {
    "id": "0bde3275-6327-489d-b3c4-e7bafd1a9f83"
   },
   "source": [
    "Let's import the dataframe summarizing the data on books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac18f3b-3110-4c52-b384-208993c292b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1678901610230,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "3ac18f3b-3110-4c52-b384-208993c292b2",
    "outputId": "e3043a33-0261-4f0f-f16e-0c3aa5c11a15",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>length</th>\n",
       "      <th>year</th>\n",
       "      <th>group</th>\n",
       "      <th>chunk_numbers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34204</th>\n",
       "      <td>La petite Fadette</td>\n",
       "      <td>George Sand</td>\n",
       "      <td>331806.0</td>\n",
       "      <td>1869.0</td>\n",
       "      <td>34204</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24850</th>\n",
       "      <td>Lourdes</td>\n",
       "      <td>Émile Zola</td>\n",
       "      <td>1068528.0</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>24850</td>\n",
       "      <td>680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41054</th>\n",
       "      <td>Cours familier de Littérature - Volume 07</td>\n",
       "      <td>Alphonse de Lamartine</td>\n",
       "      <td>505709.0</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>41054</td>\n",
       "      <td>315.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63794</th>\n",
       "      <td>La comédie de celui qui épousa une femme muette</td>\n",
       "      <td>Anatole France</td>\n",
       "      <td>48980.0</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>63794</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12367</th>\n",
       "      <td>Le péché de Monsieur Antoine, Tome 1</td>\n",
       "      <td>George Sand</td>\n",
       "      <td>559615.0</td>\n",
       "      <td>1845.0</td>\n",
       "      <td>Antoine</td>\n",
       "      <td>353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13668</th>\n",
       "      <td>Le château des Désertes</td>\n",
       "      <td>George Sand</td>\n",
       "      <td>294819.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13668</td>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37604</th>\n",
       "      <td>Cours familier de Littérature - Volume 10</td>\n",
       "      <td>Alphonse de Lamartine</td>\n",
       "      <td>450970.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>37604</td>\n",
       "      <td>270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17693</th>\n",
       "      <td>La San-Felice, Tome 01</td>\n",
       "      <td>Alexandre Dumas</td>\n",
       "      <td>388396.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>San-Felice</td>\n",
       "      <td>234.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7772</th>\n",
       "      <td>Les Quarante-Cinq — Tome 3</td>\n",
       "      <td>Alexandre Dumas</td>\n",
       "      <td>459954.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valois</td>\n",
       "      <td>287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17660</th>\n",
       "      <td>L'archipel en feu</td>\n",
       "      <td>Jules Verne</td>\n",
       "      <td>362647.0</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>17660</td>\n",
       "      <td>226.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "number                                                    \n",
       "34204                                 La petite Fadette   \n",
       "24850                                           Lourdes   \n",
       "41054         Cours familier de Littérature - Volume 07   \n",
       "63794   La comédie de celui qui épousa une femme muette   \n",
       "12367              Le péché de Monsieur Antoine, Tome 1   \n",
       "...                                                 ...   \n",
       "13668                           Le château des Désertes   \n",
       "37604         Cours familier de Littérature - Volume 10   \n",
       "17693                            La San-Felice, Tome 01   \n",
       "7772                         Les Quarante-Cinq — Tome 3   \n",
       "17660                                 L'archipel en feu   \n",
       "\n",
       "                       author     length    year       group  chunk_numbers  \n",
       "number                                                                       \n",
       "34204             George Sand   331806.0  1869.0       34204          199.0  \n",
       "24850              Émile Zola  1068528.0  1894.0       24850          680.0  \n",
       "41054   Alphonse de Lamartine   505709.0  1859.0       41054          315.0  \n",
       "63794          Anatole France    48980.0  1912.0       63794           24.0  \n",
       "12367             George Sand   559615.0  1845.0     Antoine          353.0  \n",
       "...                       ...        ...     ...         ...            ...  \n",
       "13668             George Sand   294819.0     NaN       13668          183.0  \n",
       "37604   Alphonse de Lamartine   450970.0  1860.0       37604          270.0  \n",
       "17693         Alexandre Dumas   388396.0  1800.0  San-Felice          234.0  \n",
       "7772          Alexandre Dumas   459954.0     NaN      Valois          287.0  \n",
       "17660             Jules Verne   362647.0  1884.0       17660          226.0  \n",
       "\n",
       "[301 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "no_dupl=pd.read_csv('no_dupl.csv', index_col=0)\n",
    "no_dupl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb169bd-3759-47be-ba47-b183ae245343",
   "metadata": {
    "id": "feb169bd-3759-47be-ba47-b183ae245343"
   },
   "source": [
    "During the course, we chose our test set randomly. When we did cross-validation, the folds were also selected randomly. In this case, we have to somewhat restrict the selection.\n",
    "\n",
    "Indeed, it would be too easy to recognize authors if a test sample was from the same book or same book group as the train sample. In this case the classifier would not only have the style of the author as a clue, but also the names of the characters or the formatting of the book.\n",
    "\n",
    "If possible, we would like this to hold for the cross-validation too: the training chunks should come from different books/book groups than the validation chunks.\n",
    "\n",
    "\n",
    "To achieve this, we will define 5 book bundles and we will use predefined splits for splitting the training set in cross-validation. The test set and each of the folds in the training set will come from each of the book bundles. We try to make sure that that each book group (i.e. volumes of novels or books constituting a saga) are contained in a book bundle.\n",
    "\n",
    "This way, we can avoid that a classifier trains on \"The three musketeers\", sees \"Aramis\" in it and when it sees a chunk from \"Twenty Years After\", a book from the same saga, it simply recognizes it because \"Aramis\" figures in that chunk too. We will group all books of this saga in one of the 4+1 book bundles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e540d-3b8a-4a2f-886c-67bd04aa2fd3",
   "metadata": {
    "id": "2e5e540d-3b8a-4a2f-886c-67bd04aa2fd3"
   },
   "source": [
    "How many chunks do we have per author?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2aec92d-1ac4-4813-a2f4-92e8bfef49f5",
   "metadata": {
    "id": "c2aec92d-1ac4-4813-a2f4-92e8bfef49f5",
    "outputId": "d5f3ce68-e321-4bed-f2fd-d621b63168d4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author\n",
       "Gustave Flaubert          2680.0\n",
       "Anatole France            3662.0\n",
       "Guy de Maupassant         3896.0\n",
       "Marcel Proust             4234.0\n",
       "Victor Hugo               7006.0\n",
       "Alphonse de Lamartine     8198.0\n",
       "Jules Verne              12434.0\n",
       "Émile Zola               13435.0\n",
       "George Sand              19777.0\n",
       "Alexandre Dumas          20019.0\n",
       "Name: chunk_numbers, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_dupl.groupby(\"author\")[\"chunk_numbers\"].sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6ccc1-99c5-444d-8773-f5cf177402de",
   "metadata": {
    "id": "90d6ccc1-99c5-444d-8773-f5cf177402de",
    "tags": []
   },
   "source": [
    "We would like to generate test and train tests sets that are more or less the same size for each author. Let's try the bundles so that we can choose a test set of 300 per author and a training set of 1200 per author. We will also try to make sure that the test set comes at least eightth of the books/book groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f14a78-1b44-40d1-98a7-8c750629c7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def attempt_bundles(refs, sizes, min_sums, min_elements): #we have a references list (can be a book number list or a book group name list) with corresponding sizes, we want to make bundles with at least min_elements (a list) and at least min_sums of aggregate size (also a list)\n",
    "    bundles_nr=len(min_sums)\n",
    "    bundles=[]\n",
    "    bundle_idx=0\n",
    "    current_bundle=[]\n",
    "    ref_dict=dict(zip(refs, sizes))\n",
    "    for i, ref in enumerate(refs):\n",
    "        \n",
    "        current_bundle.append(ref)\n",
    "        \n",
    "        if (sum([ref_dict[e] for e in current_bundle])>=min_sums[bundle_idx])&(len(current_bundle)>=min_elements[bundle_idx]):\n",
    "            bundle_idx+=1\n",
    "            bundles.append(current_bundle)\n",
    "            if bundle_idx==bundles_nr:\n",
    "                bundles[-1]=current_bundle+refs[i+1:]\n",
    "                break\n",
    "            current_bundle=[]    \n",
    "    if bundle_idx==bundles_nr:\n",
    "        return bundles\n",
    "    else:\n",
    "        return 'no success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "514c8a9b-f083-4cd1-aada-700703828ece",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot select test book bundles and training book bundles with these parameters for Marcel Proust. The test book bundles will not be from different book groups\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>length</th>\n",
       "      <th>year</th>\n",
       "      <th>group</th>\n",
       "      <th>chunk_numbers</th>\n",
       "      <th>trainortest</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34204</th>\n",
       "      <td>La petite Fadette</td>\n",
       "      <td>George Sand</td>\n",
       "      <td>331806.0</td>\n",
       "      <td>1869.0</td>\n",
       "      <td>34204</td>\n",
       "      <td>199.0</td>\n",
       "      <td>valid4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24850</th>\n",
       "      <td>Lourdes</td>\n",
       "      <td>Émile Zola</td>\n",
       "      <td>1068528.0</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>24850</td>\n",
       "      <td>680.0</td>\n",
       "      <td>valid4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41054</th>\n",
       "      <td>Cours familier de Littérature - Volume 07</td>\n",
       "      <td>Alphonse de Lamartine</td>\n",
       "      <td>505709.0</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>41054</td>\n",
       "      <td>315.0</td>\n",
       "      <td>valid4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63794</th>\n",
       "      <td>La comédie de celui qui épousa une femme muette</td>\n",
       "      <td>Anatole France</td>\n",
       "      <td>48980.0</td>\n",
       "      <td>1912.0</td>\n",
       "      <td>63794</td>\n",
       "      <td>24.0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12367</th>\n",
       "      <td>Le péché de Monsieur Antoine, Tome 1</td>\n",
       "      <td>George Sand</td>\n",
       "      <td>559615.0</td>\n",
       "      <td>1845.0</td>\n",
       "      <td>Antoine</td>\n",
       "      <td>353.0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13668</th>\n",
       "      <td>Le château des Désertes</td>\n",
       "      <td>George Sand</td>\n",
       "      <td>294819.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13668</td>\n",
       "      <td>183.0</td>\n",
       "      <td>valid2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37604</th>\n",
       "      <td>Cours familier de Littérature - Volume 10</td>\n",
       "      <td>Alphonse de Lamartine</td>\n",
       "      <td>450970.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>37604</td>\n",
       "      <td>270.0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17693</th>\n",
       "      <td>La San-Felice, Tome 01</td>\n",
       "      <td>Alexandre Dumas</td>\n",
       "      <td>388396.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>San-Felice</td>\n",
       "      <td>234.0</td>\n",
       "      <td>valid1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7772</th>\n",
       "      <td>Les Quarante-Cinq — Tome 3</td>\n",
       "      <td>Alexandre Dumas</td>\n",
       "      <td>459954.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valois</td>\n",
       "      <td>287.0</td>\n",
       "      <td>valid4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17660</th>\n",
       "      <td>L'archipel en feu</td>\n",
       "      <td>Jules Verne</td>\n",
       "      <td>362647.0</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>17660</td>\n",
       "      <td>226.0</td>\n",
       "      <td>valid4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "number                                                    \n",
       "34204                                 La petite Fadette   \n",
       "24850                                           Lourdes   \n",
       "41054         Cours familier de Littérature - Volume 07   \n",
       "63794   La comédie de celui qui épousa une femme muette   \n",
       "12367              Le péché de Monsieur Antoine, Tome 1   \n",
       "...                                                 ...   \n",
       "13668                           Le château des Désertes   \n",
       "37604         Cours familier de Littérature - Volume 10   \n",
       "17693                            La San-Felice, Tome 01   \n",
       "7772                         Les Quarante-Cinq — Tome 3   \n",
       "17660                                 L'archipel en feu   \n",
       "\n",
       "                       author     length    year       group  chunk_numbers  \\\n",
       "number                                                                        \n",
       "34204             George Sand   331806.0  1869.0       34204          199.0   \n",
       "24850              Émile Zola  1068528.0  1894.0       24850          680.0   \n",
       "41054   Alphonse de Lamartine   505709.0  1859.0       41054          315.0   \n",
       "63794          Anatole France    48980.0  1912.0       63794           24.0   \n",
       "12367             George Sand   559615.0  1845.0     Antoine          353.0   \n",
       "...                       ...        ...     ...         ...            ...   \n",
       "13668             George Sand   294819.0     NaN       13668          183.0   \n",
       "37604   Alphonse de Lamartine   450970.0  1860.0       37604          270.0   \n",
       "17693         Alexandre Dumas   388396.0  1800.0  San-Felice          234.0   \n",
       "7772          Alexandre Dumas   459954.0     NaN      Valois          287.0   \n",
       "17660             Jules Verne   362647.0  1884.0       17660          226.0   \n",
       "\n",
       "       trainortest  \n",
       "number              \n",
       "34204       valid4  \n",
       "24850       valid4  \n",
       "41054       valid4  \n",
       "63794         test  \n",
       "12367         test  \n",
       "...            ...  \n",
       "13668       valid2  \n",
       "37604         test  \n",
       "17693       valid1  \n",
       "7772        valid4  \n",
       "17660       valid4  \n",
       "\n",
       "[301 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a not very elegant and inefficient (although not at all bottleneck-creating) code but gets the job done.\n",
    "# For each author, first we try to get split the books into 4 book bundles for validation and a test book bundle in a way that\n",
    "# the bundles respect the book group boundaries (there is no book group with one book in a bundle and another book another bundle\n",
    "# which would lead to train and validate/test on chunks from the same book groups)\n",
    "\n",
    "# For this, we try a permutation of books, check if the first book is enough for a test book bundle (i.e. has more than setsize[\"test\"] chunks),\n",
    "# if not, we try with adding the second book, if still not enough, the we also add the third etc. When we are ready with the test bundle, we add books in a similar\n",
    "# way to the first training book bundle etc.\n",
    "# if the permutation fails, we try another permutation, after 10 tries we give up.\n",
    "\n",
    "# When it does not work (for Proust) we relax the condition and only try to make sure that the test bundle has different book groups than the other bundles.\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "no_dupl.loc[:,\"trainortest\"]=\"None\"\n",
    "\n",
    "np.random.seed(42)\n",
    "for author, author_df in no_dupl.groupby('author'):\n",
    "    chunks_per_group=author_df.groupby(\"group\")[\"chunk_numbers\"].sum()\n",
    "    success=True\n",
    "    for i in range(10):\n",
    "        perm=chunks_per_group.loc[np.random.permutation(chunks_per_group.index)]\n",
    "        output=attempt_bundles(refs=list(perm.index), sizes=list(perm.values), min_sums=[300]*5, min_elements=[len(perm)/8]*5)\n",
    "        if output!='no success':\n",
    "            for idx, bundle in enumerate(output):\n",
    "                no_dupl.loc[no_dupl['group'].isin(bundle), 'trainortest']=['test', 'valid1', 'valid2', 'valid3', 'valid4'][idx] \n",
    "            success=True\n",
    "            break\n",
    "        else:\n",
    "            success=False\n",
    "    if not success:\n",
    "        print(\"I cannot select test book bundles and training book bundles with these parameters for {}. The test book bundles will not be from different book groups\".format(author))\n",
    "        for j in range(30):\n",
    "            perm=chunks_per_group.loc[np.random.permutation(chunks_per_group.index)]\n",
    "            output_test=attempt_bundles(refs=list(perm.index), sizes=list(perm.values), min_sums=[300, 1200], min_elements=[1, 1]) #test book bundle creation\n",
    "            #print('test', perm.values, [len(perm)/8, len(perm)/8*4], output_test)\n",
    "            if output_test!='no success':\n",
    "                sizes_train=author_df.loc[author_df['group'].isin(output_test[1]), 'chunk_numbers']\n",
    "                output_train=attempt_bundles(refs=list(sizes_train.index), sizes=list(sizes_train.values), min_sums=[300]*4, min_elements=[len(sizes_train)/8]*4) #train book bundle creation, relaxed version\n",
    "                #print(output_train)\n",
    "                if output_train!='no success':\n",
    "                    no_dupl.loc[no_dupl['group'].isin(output_test[0]), 'trainortest']='test'\n",
    "                    for idx, bundle in enumerate(output_train):\n",
    "                        no_dupl.loc[no_dupl.index.isin(bundle), 'trainortest']=['valid1', 'valid2', 'valid3', 'valid4'][idx] \n",
    "                    success=True\n",
    "                    break\n",
    "            else:\n",
    "                success=False\n",
    "        if not success:\n",
    "            print(\"I cannot select test and validation books with these parameters for {} at all.\".format(author))\n",
    "            #break\n",
    "no_dupl\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YSw3QDkC6zkn",
   "metadata": {
    "id": "YSw3QDkC6zkn"
   },
   "source": [
    "Indeed we now have at least 300 chunks in every book bundle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "_Ey5MPlUxkYz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1678725752188,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "_Ey5MPlUxkYz",
    "outputId": "206284d4-e311-44f5-fca0-866b25596e31",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(no_dupl.groupby([\"author\",\"trainortest\"])[\"chunk_numbers\"].sum()<300).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e58b8de-268a-480e-a7d1-cd7810f655d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "And only one book group is in more than one bundle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b84220e-ecdf-47ca-91a9-0d66bc66f382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Marcel Proust', 'Recherche') ['valid1' 'valid2' 'valid3' 'valid4']\n"
     ]
    }
   ],
   "source": [
    "for a, b in no_dupl.groupby(['author', 'group'])['trainortest']:\n",
    "    if len(b.unique())>1:\n",
    "        print(a, b.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2709c5-f776-4906-80a2-b42f685b63c4",
   "metadata": {
    "id": "5c2709c5-f776-4906-80a2-b42f685b63c4"
   },
   "source": [
    "While the selection of books/book groups that form the book bundles  (from which the chunks constituting the test set and the different folds of the training set will be selected) is somewhat random, in some cases the randomness is quite limited. For Proust, the only way to make sure we have enough test chunks is to use the two books outside of In search of lost time for testing. This limited randomness is a compromise we have to make if we want our classification algorithm to use only style elements and not names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7afc674-bf90-4911-a359-a29f6eaec90b",
   "metadata": {
    "id": "c7afc674-bf90-4911-a359-a29f6eaec90b",
    "tags": []
   },
   "source": [
    "## Creation of train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40206662-283f-43a8-82d6-4c4f6ed3484c",
   "metadata": {
    "id": "40206662-283f-43a8-82d6-4c4f6ed3484c"
   },
   "source": [
    "Let's create our train and test folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74954f56-a8a6-426f-a79b-e5a92bf25cfa",
   "metadata": {
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1678902241925,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "74954f56-a8a6-426f-a79b-e5a92bf25cfa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for folder in [\"valid1\", \"valid2\", \"valid3\", \"valid4\", \"test\"]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08350d23-42a6-48e6-a8d6-83647b1232b9",
   "metadata": {
    "id": "08350d23-42a6-48e6-a8d6-83647b1232b9"
   },
   "source": [
    "Now let's define a function to randomly select a given number of chunks from a selection of an author's books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4cc133-9fa1-41d7-93f4-5ad5db507610",
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1678902246640,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "5f4cc133-9fa1-41d7-93f4-5ad5db507610",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "def dataset_creator(author, dataset_size, trainortest):\n",
    "    book_numbers=no_dupl[(no_dupl[\"author\"]==author)&(no_dupl[\"trainortest\"]==trainortest)].index\n",
    "    \n",
    "    chunks=[]\n",
    "    chunk_book_numbers=[]\n",
    "    for n in book_numbers:\n",
    "        \n",
    "        filename=os.path.join(\"chunkified\", author, str(n))\n",
    "        with open(filename, 'r') as f:\n",
    "            chunksinfile=f.read().split(\"\\n\\t\\t\\n\")\n",
    "            if author=='Alphonse de Lamartine':\n",
    "                chunksinfile=[chunk for chunk in chunksinfile\n",
    "                              if (\"LAMARTINE\" not in chunk) and (\"ENTRETIEN\" not in chunk) and (\"Rouge frères, Dunon et Fresné\" not in chunk)]\n",
    "            chunks+=chunksinfile \n",
    "            chunk_book_numbers+=[n]*len(chunksinfile)\n",
    "    sample=random.sample(range(len(chunks)), dataset_size)\n",
    "    chunk_sample=[chunks[i] for i in sample]\n",
    "    book_number_sample=[chunk_book_numbers[i] for i in sample]\n",
    "    return chunk_sample, book_number_sample\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34082a2d-a951-4562-abbd-3fe0c85bff59",
   "metadata": {
    "executionInfo": {
     "elapsed": 59567,
     "status": "ok",
     "timestamp": 1678902309890,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "34082a2d-a951-4562-abbd-3fe0c85bff59",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for t in [\"valid1\", \"valid2\", \"valid3\", \"valid4\", \"test\"]:\n",
    "    if t==\"test\":\n",
    "        ds_size=300\n",
    "    else:\n",
    "        ds_size=300\n",
    "    all_chunk_book_numbers=pd.DataFrame(columns=no_dupl[\"author\"].unique())\n",
    "    for a in no_dupl[\"author\"].unique():\n",
    "\n",
    "        chunks, chunk_book_numbers=dataset_creator(a, ds_size, t)\n",
    "        all_chunk_book_numbers[a]=chunk_book_numbers\n",
    "        content=\"\\n\\t\\t\\n\".join(chunks)\n",
    "        filename=os.path.join(t, a)\n",
    "        if not os.path.isfile(filename):\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(content)\n",
    "    filename=os.path.join(t, 'chunk_numbers.csv')\n",
    "    if not os.path.isfile(filename):\n",
    "        all_chunk_book_numbers.to_csv(filename)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf6rb9LK8FpI",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1678902309890,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "bf6rb9LK8FpI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_dupl.to_csv('no_dupl.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b58a2-403c-479a-a13d-98ce122a8af3",
   "metadata": {
    "id": "755b58a2-403c-479a-a13d-98ce122a8af3"
   },
   "source": [
    "## New manuscript or lazy student? - further explanation for the bundles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546aa043-3461-4eea-ae0c-6e5977987d1d",
   "metadata": {
    "id": "546aa043-3461-4eea-ae0c-6e5977987d1d"
   },
   "source": [
    "There are two ways to consider our style classification task:\n",
    "    <ul>\n",
    "    <li>as a task for a <b>lazy literary student</b>. Let's say a student has read 80% of the bibliography, i.e 80% of the chunks. She is then given a chunk and has to to determine the author. In this case, using the protagonists' names or a certain formatting choice (e.g. which kind of quotation marks the book has) to recognise the author is fair game. The training set can contain \"Les trois mousquetaires\" and the test set can contain \"Vingt ans apres\" as we can get points if we can recognize that \"Athos\" means Dumas. </li>\n",
    "    <li>as a task to identify the author of a <b>new manuscript</b>. Let's say we have found a page from an unknown book and we want to recognize the author. It is reasonable to assume that the style (including the themes evoked) will be similar, but it will not be a new volume of a known book or a sequel to a saga. We cannot use the names and the formatting. While we only simulate this task and the simulation is far from perfect, this is what we aim for and that is why we try to eliminate formatting and named entity clues. </li>\n",
    "    </ul>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
