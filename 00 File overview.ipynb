{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa5188b-f043-4463-b6e6-817479bb72e3",
   "metadata": {},
   "source": [
    "# 00 File overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce785473-d995-4484-acff-d185b1506935",
   "metadata": {},
   "source": [
    "The environment is contained in the <TT><b>adam_toth_epfl_ml2.yml</b></TT> file.\nThe french spacy model has to be installed with <TT><b>python -m spacy download fr_core_news_lg</b></TT>.\n",
    "\n",
    "All of the files used by the project are already contained in the repository, except the gutenbergindex.db, which is extracted (see below), because of github's limit. The files in the folders are not overwritten, i.e. you have to delete the folders in question if you want to see the function that creates or fills the folder. On the other hand, the files in the root are overwritten.\n",
    "\n",
    "<TT><b>01 Text selection and download.ipynb </b></TT> extracts <TT><b>gutenbergindex.db.zip</b></TT> (SQL catalog of the Gutenberg Project) and downloads the raw texts from the Gutenberg Project and puts them in the <TT><b>rawtext</b></TT> folder, in the author subfolders. It creates a <TT><b>sel_gut.csv</b></TT> for a pandas catalog of the selected books.\n",
    "\n",
    "\n",
    "<TT><b>02 Text cleanup and data exploration.ipynb</b></TT> explores the data and finds the duplicates. It has one longer cell (cca. 12 minutes on my iMac2019) for text comparison. It creates shortened, cleaned texts in the author subfolders of the <TT><b>stripped_filtered</b></TT> folder and a <TT><b>no_dupl.csv</b></TT> file for a pandas catalog df, without duplicates.\n",
    "\n",
    "<TT><b>03 Further data exploration and chunking.ipynb</b></TT> further explores the data and groups the books together. It has two longer cells (cca. 30 minutes each) for spacy analysis. It creates chunks from each book and puts them in the author subfolders of the <TT><b>chunkified</b></TT> folder. It updates the <TT><b>no_dupl.csv</b></TT> file.\n",
    "\n",
    "<TT><b>04 Creation of train and test sets.ipynb</b></TT> defines a function to split the books into 4 book bundles for training (valid1 to valid4) and 1 book bundle for testing (test). From each book bundle and for each author, 300 chunks are selected randomly which are put in the author files in the <TT><b>valid1, valid2, valid3, valid4</b></TT> and <TT><b>test</b></TT> folder. Each folder corresponds to the chunks in a fold in the cross-validation. Each of these folders also contains a <TT><b>chunk_numbers.csv</b></TT> file which show from which book the chunks were selected. The notebook updates the <TT><b>no_dupl.csv</b></TT> file.\n",
    "\n",
    "<TT><b>05 Feature extraction.ipynb</b></TT> creates embeddings for almost each chunk. It takes a long time, cca. 6 hours on my computer, if the files are not there already. Skipped chunk numbers are contained in the <TT><b>too_big.pkl</b></TT> file. The embeddings themselves are in the <TT><b>embeddings</b></TT> folder, for each embedding (<TT><b>w2v, fastText, uni_sent_enc, camembert-wiki, camembert-large</b></TT>) and for each fold (the folds are again <TT><b>valid1, valid2, valid3, valid4, test</b></TT>).\n",
    "\n",
    "<TT><b>006 PCA, Logistic regression and random forest, neural networks, error analysis, conclusions.ipynb</b></TT> visualizes the vectors and creates classification models with different methods and embeddings. It analyzes the results and errors and draws some lessons. It takes about 2 hours to run. It saves the winning model in <TT><b>logreg_model.sav</b></TT> and the accuracy scores in the <TT><b>results.csv</b></TT> file.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
