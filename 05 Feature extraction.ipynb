{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b8baf0e-bfd5-4022-854f-c4622d67be18",
   "metadata": {
    "id": "5b8baf0e-bfd5-4022-854f-c4622d67be18"
   },
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515506eb-af6d-4096-8099-94ce5f4191b3",
   "metadata": {
    "id": "515506eb-af6d-4096-8099-94ce5f4191b3"
   },
   "source": [
    "<i>Attention, the embeddings take quite some time to generate. On my computer (iMac 2019, 3,7 GHz 6-Core Intel Core i5), it took about 6 hours. The embedding files are already in the embeddings folder and the final generate_embeddings function in the last cell only generates the embeddings if you specify overwrite=True.</i>\n",
    "\n",
    "In this notebook we produce five embeddings for each chunk:\n",
    "<ul>\n",
    "<li><b>word2vec</b>, a classic word embedding from which we can make an embedding for our chunk by averaging over the words in the chunk</li>\n",
    "<li><b>fastText</b>, the embedding used in the French spacy model we have been using</li>\n",
    "<li><b>universal sentence encoder</b>, a relatively recent and large multilingual embedding</li>\n",
    "<li><b>camembert-large</b>, a relatively recent and large and French-specific embedding</li>\n",
    "<li><b>camembert-base-wikipedia-4gb</b>, a smaller French specific embedding</li>\n",
    "</ul>\n",
    "There is not easy to find a ready-made word2vec embedding in French, as it is no longer in fashion. We train a model based on our train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36641c81-9a07-46d3-a6c3-2cfb06a172f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5724820, 6432490)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize #we use an older tool for the older embedding\n",
    "from nltk import sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "no_dupl=pd.read_csv('no_dupl.csv')\n",
    "all_chunks={}\n",
    "\n",
    "for t in [\"valid1\", \"valid2\", \"valid3\", \"valid4\", 'test']:\n",
    "    all_chunks[t]=[]\n",
    "    for author in no_dupl[\"author\"].unique():\n",
    "        with open(os.path.join(t, author), 'r') as f:\n",
    "            all_chunks[t]+=f.read().split(\"\\n\\t\\t\\n\")\n",
    "\n",
    "fr_stop=stopwords.words('french')+['?', '°', '(', '.', '=', '!', ';', ')', ',', ':', \"'\", '-', '*', ']', '[', '\"', '...']\n",
    "\n",
    "def prepare(chunk):\n",
    "    sents=sent_tokenize(chunk, language='french')\n",
    "\n",
    "    return [[token.lower() for token in word_tokenize(sent, language='french') if token.lower() not in fr_stop] for sent in sents]\n",
    "\n",
    "train_chunks=all_chunks[\"valid1\"]+all_chunks[\"valid2\"]+all_chunks[\"valid3\"]+all_chunks[\"valid4\"]\n",
    "train_sents=[sent for chunk in train_chunks for sent in prepare(chunk)]\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(vector_size=200,  workers=4, sg=0)  \n",
    "w2v_model.build_vocab(train_sents)\n",
    "w2v_model.train(train_sents, total_examples=w2v_model.corpus_count, epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e201c83d-343f-4261-8c46-c0d315d54ed4",
   "metadata": {
    "id": "e201c83d-343f-4261-8c46-c0d315d54ed4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb_models=[\"w2v\",\"fastText\", \"uni_sent_enc\", \"camembert-wiki\", \"camembert-large\"]\n",
    "vector_sizes=[200, 300, 512, 768, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f22f993-ae53-44ef-9f04-f87c705ea7c9",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1678906814279,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "6f22f993-ae53-44ef-9f04-f87c705ea7c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' #we supress the tensorflow info messages\n",
    "import torch\n",
    "import spacy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e63abbd-ae6c-4222-b0aa-8eae71f7bbeb",
   "metadata": {
    "id": "8e63abbd-ae6c-4222-b0aa-8eae71f7bbeb"
   },
   "source": [
    "We import spacy for fastText:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07909bd-5670-4212-9334-310756af88fa",
   "metadata": {
    "executionInfo": {
     "elapsed": 10849,
     "status": "ok",
     "timestamp": 1678906892752,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "a07909bd-5670-4212-9334-310756af88fa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0333ae-7677-4e71-ad08-02ddfb86bc77",
   "metadata": {
    "id": "9e0333ae-7677-4e71-ad08-02ddfb86bc77",
    "tags": []
   },
   "source": [
    "We load the universal sentence encoder from the tensorflow hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f73bc5-586d-48e1-b784-3eaaa56d6a4d",
   "metadata": {
    "executionInfo": {
     "elapsed": 18901,
     "status": "ok",
     "timestamp": 1678906911644,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "c3f73bc5-586d-48e1-b784-3eaaa56d6a4d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "uni_sent_enc = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f3aa7-b7ad-452d-9096-dacad7b34675",
   "metadata": {
    "id": "9a8f3aa7-b7ad-452d-9096-dacad7b34675"
   },
   "source": [
    "We load the two Camembert model using the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eae2373-fbd2-42e0-b5bc-b6a165c47fdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27583,
     "status": "ok",
     "timestamp": 1678906968817,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "6eae2373-fbd2-42e0-b5bc-b6a165c47fdf",
    "outputId": "16c80082-77f7-4663-e172-26d116e295ce",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert/camembert-large were not used when initializing CamembertModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at camembert/camembert-base-wikipedia-4gb were not used when initializing CamembertModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertModel, CamembertTokenizer\n",
    "\n",
    "tokenizer={}\n",
    "camembert={}\n",
    "tokenizer[\"camembert-large\"] = CamembertTokenizer.from_pretrained(\"camembert/camembert-large\")\n",
    "camembert[\"camembert-large\"] = CamembertModel.from_pretrained(\"camembert/camembert-large\")\n",
    "\n",
    "tokenizer[\"camembert-wiki\"] = CamembertTokenizer.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\")\n",
    "camembert[\"camembert-wiki\"] = CamembertModel.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\")\n",
    "\n",
    "camembert[\"camembert-large\"].eval();\n",
    "camembert[\"camembert-wiki\"].eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb419c7-349d-40b8-86c9-d1d7825e16df",
   "metadata": {
    "id": "1cb419c7-349d-40b8-86c9-d1d7825e16df"
   },
   "source": [
    "Before we start extracting features, we want to make sure that the chunks are not too big. Camembert has a limit of 512 tokens. What a token is depends on the tokenizer, which is different for the large and the wiki model. Let's see if we can ignore the chunks that are too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ffbffe-dc6c-4f5e-a330-c7151e80dd99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86458,
     "status": "ok",
     "timestamp": 1678907055263,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "94ffbffe-dc6c-4f5e-a330-c7151e80dd99",
    "outputId": "73b85d7f-e16b-4065-820e-784526e839e9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have to skip 1 chunk(s) in the valid4 set for George Sand\n",
      "We have to skip 2 chunk(s) in the valid4 set for Victor Hugo\n",
      "We have to skip 4 chunk(s) in the test set for Victor Hugo\n",
      "We have to skip 1 chunk(s) in the test set for Marcel Proust\n"
     ]
    }
   ],
   "source": [
    "def chunk_length_measurer(filename, emb_model):\n",
    "    with open(filename, 'r') as f:\n",
    "        chunks=f.read().split(\"\\n\\t\\t\\n\")\n",
    "        lengths=[]\n",
    "        for i, ch in enumerate(chunks):\n",
    "            tokenized=tokenizer[emb_model].tokenize(ch)\n",
    "            encoded=tokenizer[emb_model].encode(tokenized)\n",
    "            encoded = torch.tensor(encoded).unsqueeze(0)\n",
    "            lengths.append(encoded.shape[1])\n",
    "    return lengths\n",
    "chunk_lengths={}\n",
    "too_big={}\n",
    "for t in [\"valid1\", \"valid2\", \"valid3\", \"valid4\", \"test\"]:\n",
    "    chunk_lengths[t]={}\n",
    "    too_big[t]={}\n",
    "    for a in no_dupl[\"author\"].unique():\n",
    "        chunk_lengths[t][a]=max(chunk_length_measurer(os.path.join(t, a), \"camembert-wiki\"),chunk_length_measurer(os.path.join(t, a), \"camembert-large\"))\n",
    "        too_big[t][a]=[]\n",
    "        for (i, l) in enumerate(chunk_lengths[t][a]):\n",
    "            if l>512:\n",
    "                too_big[t][a]+=[i] # we save the location of the two big tokens for later use\n",
    "        if len(too_big[t][a])>0:\n",
    "            print(\"We have to skip {} chunk(s) in the {} set for {}\".format(len(too_big[t][a]), t, a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63370d3d-321b-46f0-8331-5f4a2e4303ef",
   "metadata": {
    "id": "e852fd97-36f0-4f87-ae23-124c776fe198"
   },
   "source": [
    "It looks these represent a very small fraction of the train/test set, so we will just ignore these chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853527f6-4376-4ccd-b23b-5c75f8160bea",
   "metadata": {
    "tags": []
   },
   "source": [
    "We save the the too_big dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20fd9234-aac1-4d17-a7af-1705f7c916cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"too_big.pkl\", \"wb\") as file:\n",
    "    pickle.dump(too_big, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592252cd-a437-4101-a8ba-b236c00ba827",
   "metadata": {
    "id": "e852fd97-36f0-4f87-ae23-124c776fe198"
   },
   "source": [
    "We define three functions to generate embeddings:\n",
    "<ul>\n",
    "<li><tt><b>chunk_to_vector</b></tt> for the chunk level</li>\n",
    "<li><tt><b>embedder</b></tt> for the author level to produce embeddings for all the chunks for an author in the train/test set</li>\n",
    "<li><tt><b>generate_embeddings</b></tt> for the embedding model level to generate embeddings for all authors</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17e43373-b3c2-430a-a16f-ac035388b137",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1678907055264,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "17e43373-b3c2-430a-a16f-ac035388b137",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_to_vector(ch, emb_model):\n",
    "    if emb_model=='w2v':\n",
    "        tokens=[token for sent in prepare(ch) for token in sent]\n",
    "        vectors=[w2v_model.wv.get_vector(token) for token in tokens if token in w2v_model.wv.key_to_index]\n",
    "        if not vectors:\n",
    "            vectors=np.zeros(200)\n",
    "        embedding=np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "    \n",
    "    elif emb_model==\"fastText\":\n",
    "        processed=nlp(ch)\n",
    "        embedding=processed.vector\n",
    "    elif emb_model==\"uni_sent_enc\":\n",
    "        embedding=uni_sent_enc([ch])\n",
    "    elif emb_model==\"camembert-large\" or emb_model==\"camembert-wiki\":\n",
    "        tokenized=tokenizer[emb_model].tokenize(ch)\n",
    "        encoded=tokenizer[emb_model].encode(tokenized)\n",
    "        encoded = torch.tensor(encoded).unsqueeze(0)\n",
    "        embedding=camembert[emb_model](encoded).pooler_output.detach().numpy()\n",
    "    else:\n",
    "        print(\"unknown model\")\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c556e975-1ec0-4d42-8948-fa37c7fde729",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678907055264,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "c556e975-1ec0-4d42-8948-fa37c7fde729",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def embedder(testortrain, author, emb_model, vector_size):\n",
    "    filename=os.path.join(testortrain, author)\n",
    "    print(filename)\n",
    "    print(time.ctime())\n",
    "    j=0\n",
    "    with open(filename, 'r') as f:\n",
    "        chunks=f.read().split(\"\\n\\t\\t\\n\")\n",
    "    length=len(chunks)\n",
    "    embeddings = np.zeros((length, vector_size))\n",
    "    for i, ch in enumerate(chunks):\n",
    "        if i in too_big[testortrain][author]: #we skip the chunks that are too big for one of the camembert models\n",
    "             pass\n",
    "        else:\n",
    "            embedding=chunk_to_vector(ch, emb_model)\n",
    "            embeddings[j]=embedding\n",
    "            j+=1\n",
    "    embeddings=embeddings[:j]\n",
    "    labels=[author]*j\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae84fde8-44a1-48a6-a864-ce4f28a52100",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678907055264,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "ae84fde8-44a1-48a6-a864-ce4f28a52100",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(emb_model, vector_size, t, filename, overwrite=False):\n",
    "    if (not os.path.isfile(filename)) or overwrite==True:\n",
    "        emb={}\n",
    "        lab={}\n",
    "        X={}\n",
    "        y={}\n",
    "        all_authors=no_dupl[\"author\"].unique()\n",
    "        for a in all_authors:\n",
    "            emb[a], lab[a]=embedder(t, a, emb_model, vector_size)\n",
    "        X=np.vstack([emb[a] for a in all_authors])          \n",
    "        y=np.hstack([np.array(lab[a]) for a in all_authors]) \n",
    "\n",
    "        np.savez(filename, X=X, y=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc62a6-0f6f-46cb-88d5-ca72c42b8c52",
   "metadata": {
    "id": "4bbc62a6-0f6f-46cb-88d5-ca72c42b8c52"
   },
   "source": [
    "Finally we run the last one for all five embedding models (as mentioned, the last argument should be changed to overwrite=True and it takes quite some time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be2373db-f1ad-4749-a9d4-ec14c246adc7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18143707,
     "status": "ok",
     "timestamp": 1678925252537,
     "user": {
      "displayName": "Adam Toth",
      "userId": "14214935299002321847"
     },
     "user_tz": -60
    },
    "id": "be2373db-f1ad-4749-a9d4-ec14c246adc7",
    "outputId": "4890f765-790a-4f9a-9638-e3aee493a753",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for embedding w2v\n",
      "valid1/George Sand\n",
      "Sat Apr  1 19:52:03 2023\n",
      "valid1/Émile Zola\n",
      "Sat Apr  1 19:52:04 2023\n",
      "valid1/Alphonse de Lamartine\n",
      "Sat Apr  1 19:52:04 2023\n",
      "valid1/Anatole France\n",
      "Sat Apr  1 19:52:05 2023\n",
      "valid1/Jules Verne\n",
      "Sat Apr  1 19:52:05 2023\n",
      "valid1/Victor Hugo\n",
      "Sat Apr  1 19:52:06 2023\n",
      "valid1/Guy de Maupassant\n",
      "Sat Apr  1 19:52:06 2023\n",
      "valid1/Alexandre Dumas\n",
      "Sat Apr  1 19:52:07 2023\n",
      "valid1/Marcel Proust\n",
      "Sat Apr  1 19:52:07 2023\n",
      "valid1/Gustave Flaubert\n",
      "Sat Apr  1 19:52:08 2023\n",
      "valid2/George Sand\n",
      "Sat Apr  1 19:52:08 2023\n",
      "valid2/Émile Zola\n",
      "Sat Apr  1 19:52:09 2023\n",
      "valid2/Alphonse de Lamartine\n",
      "Sat Apr  1 19:52:09 2023\n",
      "valid2/Anatole France\n",
      "Sat Apr  1 19:52:10 2023\n",
      "valid2/Jules Verne\n",
      "Sat Apr  1 19:52:10 2023\n",
      "valid2/Victor Hugo\n",
      "Sat Apr  1 19:52:11 2023\n",
      "valid2/Guy de Maupassant\n",
      "Sat Apr  1 19:52:12 2023\n",
      "valid2/Alexandre Dumas\n",
      "Sat Apr  1 19:52:12 2023\n",
      "valid2/Marcel Proust\n",
      "Sat Apr  1 19:52:13 2023\n",
      "valid2/Gustave Flaubert\n",
      "Sat Apr  1 19:52:13 2023\n",
      "valid3/George Sand\n",
      "Sat Apr  1 19:52:13 2023\n",
      "valid3/Émile Zola\n",
      "Sat Apr  1 19:52:14 2023\n",
      "valid3/Alphonse de Lamartine\n",
      "Sat Apr  1 19:52:14 2023\n",
      "valid3/Anatole France\n",
      "Sat Apr  1 19:52:15 2023\n",
      "valid3/Jules Verne\n",
      "Sat Apr  1 19:52:15 2023\n",
      "valid3/Victor Hugo\n",
      "Sat Apr  1 19:52:16 2023\n",
      "valid3/Guy de Maupassant\n",
      "Sat Apr  1 19:52:16 2023\n",
      "valid3/Alexandre Dumas\n",
      "Sat Apr  1 19:52:17 2023\n",
      "valid3/Marcel Proust\n",
      "Sat Apr  1 19:52:17 2023\n",
      "valid3/Gustave Flaubert\n",
      "Sat Apr  1 19:52:18 2023\n",
      "valid4/George Sand\n",
      "Sat Apr  1 19:52:18 2023\n",
      "valid4/Émile Zola\n",
      "Sat Apr  1 19:52:19 2023\n",
      "valid4/Alphonse de Lamartine\n",
      "Sat Apr  1 19:52:19 2023\n",
      "valid4/Anatole France\n",
      "Sat Apr  1 19:52:20 2023\n",
      "valid4/Jules Verne\n",
      "Sat Apr  1 19:52:20 2023\n",
      "valid4/Victor Hugo\n",
      "Sat Apr  1 19:52:21 2023\n",
      "valid4/Guy de Maupassant\n",
      "Sat Apr  1 19:52:21 2023\n",
      "valid4/Alexandre Dumas\n",
      "Sat Apr  1 19:52:22 2023\n",
      "valid4/Marcel Proust\n",
      "Sat Apr  1 19:52:22 2023\n",
      "valid4/Gustave Flaubert\n",
      "Sat Apr  1 19:52:23 2023\n",
      "test/George Sand\n",
      "Sat Apr  1 19:52:23 2023\n",
      "test/Émile Zola\n",
      "Sat Apr  1 19:52:24 2023\n",
      "test/Alphonse de Lamartine\n",
      "Sat Apr  1 19:52:24 2023\n",
      "test/Anatole France\n",
      "Sat Apr  1 19:52:25 2023\n",
      "test/Jules Verne\n",
      "Sat Apr  1 19:52:25 2023\n",
      "test/Victor Hugo\n",
      "Sat Apr  1 19:52:26 2023\n",
      "test/Guy de Maupassant\n",
      "Sat Apr  1 19:52:26 2023\n",
      "test/Alexandre Dumas\n",
      "Sat Apr  1 19:52:27 2023\n",
      "test/Marcel Proust\n",
      "Sat Apr  1 19:52:27 2023\n",
      "test/Gustave Flaubert\n",
      "Sat Apr  1 19:52:28 2023\n",
      "Generating embeddings for embedding fastText\n",
      "valid1/George Sand\n",
      "Sat Apr  1 19:52:28 2023\n",
      "valid1/Émile Zola\n",
      "Sat Apr  1 19:52:38 2023\n",
      "valid1/Alphonse de Lamartine\n",
      "Sat Apr  1 19:52:47 2023\n",
      "valid1/Anatole France\n",
      "Sat Apr  1 19:52:54 2023\n",
      "valid1/Jules Verne\n",
      "Sat Apr  1 19:53:02 2023\n",
      "valid1/Victor Hugo\n",
      "Sat Apr  1 19:53:09 2023\n",
      "valid1/Guy de Maupassant\n",
      "Sat Apr  1 19:53:17 2023\n",
      "valid1/Alexandre Dumas\n",
      "Sat Apr  1 19:53:24 2023\n",
      "valid1/Marcel Proust\n",
      "Sat Apr  1 19:53:30 2023\n",
      "valid1/Gustave Flaubert\n",
      "Sat Apr  1 19:53:37 2023\n",
      "valid2/George Sand\n",
      "Sat Apr  1 19:53:44 2023\n",
      "valid2/Émile Zola\n",
      "Sat Apr  1 19:53:50 2023\n",
      "valid2/Alphonse de Lamartine\n",
      "Sat Apr  1 19:53:57 2023\n",
      "valid2/Anatole France\n",
      "Sat Apr  1 19:54:04 2023\n",
      "valid2/Jules Verne\n",
      "Sat Apr  1 19:54:11 2023\n",
      "valid2/Victor Hugo\n",
      "Sat Apr  1 19:54:19 2023\n",
      "valid2/Guy de Maupassant\n",
      "Sat Apr  1 19:54:27 2023\n",
      "valid2/Alexandre Dumas\n",
      "Sat Apr  1 19:54:34 2023\n",
      "valid2/Marcel Proust\n",
      "Sat Apr  1 19:54:40 2023\n",
      "valid2/Gustave Flaubert\n",
      "Sat Apr  1 19:54:47 2023\n",
      "valid3/George Sand\n",
      "Sat Apr  1 19:54:53 2023\n",
      "valid3/Émile Zola\n",
      "Sat Apr  1 19:55:00 2023\n",
      "valid3/Alphonse de Lamartine\n",
      "Sat Apr  1 19:55:06 2023\n",
      "valid3/Anatole France\n",
      "Sat Apr  1 19:55:12 2023\n",
      "valid3/Jules Verne\n",
      "Sat Apr  1 19:55:19 2023\n",
      "valid3/Victor Hugo\n",
      "Sat Apr  1 19:55:25 2023\n",
      "valid3/Guy de Maupassant\n",
      "Sat Apr  1 19:55:31 2023\n",
      "valid3/Alexandre Dumas\n",
      "Sat Apr  1 19:55:38 2023\n",
      "valid3/Marcel Proust\n",
      "Sat Apr  1 19:55:44 2023\n",
      "valid3/Gustave Flaubert\n",
      "Sat Apr  1 19:55:50 2023\n",
      "valid4/George Sand\n",
      "Sat Apr  1 19:55:57 2023\n",
      "valid4/Émile Zola\n",
      "Sat Apr  1 19:56:03 2023\n",
      "valid4/Alphonse de Lamartine\n",
      "Sat Apr  1 19:56:09 2023\n",
      "valid4/Anatole France\n",
      "Sat Apr  1 19:56:16 2023\n",
      "valid4/Jules Verne\n",
      "Sat Apr  1 19:56:22 2023\n",
      "valid4/Victor Hugo\n",
      "Sat Apr  1 19:56:29 2023\n",
      "valid4/Guy de Maupassant\n",
      "Sat Apr  1 19:56:35 2023\n",
      "valid4/Alexandre Dumas\n",
      "Sat Apr  1 19:56:41 2023\n",
      "valid4/Marcel Proust\n",
      "Sat Apr  1 19:56:48 2023\n",
      "valid4/Gustave Flaubert\n",
      "Sat Apr  1 19:56:54 2023\n",
      "test/George Sand\n",
      "Sat Apr  1 19:57:00 2023\n",
      "test/Émile Zola\n",
      "Sat Apr  1 19:57:07 2023\n",
      "test/Alphonse de Lamartine\n",
      "Sat Apr  1 19:57:13 2023\n",
      "test/Anatole France\n",
      "Sat Apr  1 19:57:19 2023\n",
      "test/Jules Verne\n",
      "Sat Apr  1 19:57:25 2023\n",
      "test/Victor Hugo\n",
      "Sat Apr  1 19:57:31 2023\n",
      "test/Guy de Maupassant\n",
      "Sat Apr  1 19:57:37 2023\n",
      "test/Alexandre Dumas\n",
      "Sat Apr  1 19:57:44 2023\n",
      "test/Marcel Proust\n",
      "Sat Apr  1 19:57:50 2023\n",
      "test/Gustave Flaubert\n",
      "Sat Apr  1 19:57:56 2023\n",
      "Generating embeddings for embedding uni_sent_enc\n",
      "valid1/George Sand\n",
      "Sat Apr  1 19:58:03 2023\n",
      "valid1/Émile Zola\n",
      "Sat Apr  1 19:58:05 2023\n",
      "valid1/Alphonse de Lamartine\n",
      "Sat Apr  1 19:58:06 2023\n",
      "valid1/Anatole France\n",
      "Sat Apr  1 19:58:06 2023\n",
      "valid1/Jules Verne\n",
      "Sat Apr  1 19:58:07 2023\n",
      "valid1/Victor Hugo\n",
      "Sat Apr  1 19:58:07 2023\n",
      "valid1/Guy de Maupassant\n",
      "Sat Apr  1 19:58:08 2023\n",
      "valid1/Alexandre Dumas\n",
      "Sat Apr  1 19:58:08 2023\n",
      "valid1/Marcel Proust\n",
      "Sat Apr  1 19:58:09 2023\n",
      "valid1/Gustave Flaubert\n",
      "Sat Apr  1 19:58:10 2023\n",
      "valid2/George Sand\n",
      "Sat Apr  1 19:58:10 2023\n",
      "valid2/Émile Zola\n",
      "Sat Apr  1 19:58:11 2023\n",
      "valid2/Alphonse de Lamartine\n",
      "Sat Apr  1 19:58:11 2023\n",
      "valid2/Anatole France\n",
      "Sat Apr  1 19:58:12 2023\n",
      "valid2/Jules Verne\n",
      "Sat Apr  1 19:58:12 2023\n",
      "valid2/Victor Hugo\n",
      "Sat Apr  1 19:58:13 2023\n",
      "valid2/Guy de Maupassant\n",
      "Sat Apr  1 19:58:14 2023\n",
      "valid2/Alexandre Dumas\n",
      "Sat Apr  1 19:58:14 2023\n",
      "valid2/Marcel Proust\n",
      "Sat Apr  1 19:58:15 2023\n",
      "valid2/Gustave Flaubert\n",
      "Sat Apr  1 19:58:15 2023\n",
      "valid3/George Sand\n",
      "Sat Apr  1 19:58:16 2023\n",
      "valid3/Émile Zola\n",
      "Sat Apr  1 19:58:16 2023\n",
      "valid3/Alphonse de Lamartine\n",
      "Sat Apr  1 19:58:17 2023\n",
      "valid3/Anatole France\n",
      "Sat Apr  1 19:58:17 2023\n",
      "valid3/Jules Verne\n",
      "Sat Apr  1 19:58:18 2023\n",
      "valid3/Victor Hugo\n",
      "Sat Apr  1 19:58:18 2023\n",
      "valid3/Guy de Maupassant\n",
      "Sat Apr  1 19:58:19 2023\n",
      "valid3/Alexandre Dumas\n",
      "Sat Apr  1 19:58:19 2023\n",
      "valid3/Marcel Proust\n",
      "Sat Apr  1 19:58:20 2023\n",
      "valid3/Gustave Flaubert\n",
      "Sat Apr  1 19:58:20 2023\n",
      "valid4/George Sand\n",
      "Sat Apr  1 19:58:21 2023\n",
      "valid4/Émile Zola\n",
      "Sat Apr  1 19:58:21 2023\n",
      "valid4/Alphonse de Lamartine\n",
      "Sat Apr  1 19:58:22 2023\n",
      "valid4/Anatole France\n",
      "Sat Apr  1 19:58:22 2023\n",
      "valid4/Jules Verne\n",
      "Sat Apr  1 19:58:23 2023\n",
      "valid4/Victor Hugo\n",
      "Sat Apr  1 19:58:23 2023\n",
      "valid4/Guy de Maupassant\n",
      "Sat Apr  1 19:58:24 2023\n",
      "valid4/Alexandre Dumas\n",
      "Sat Apr  1 19:58:24 2023\n",
      "valid4/Marcel Proust\n",
      "Sat Apr  1 19:58:25 2023\n",
      "valid4/Gustave Flaubert\n",
      "Sat Apr  1 19:58:25 2023\n",
      "test/George Sand\n",
      "Sat Apr  1 19:58:26 2023\n",
      "test/Émile Zola\n",
      "Sat Apr  1 19:58:26 2023\n",
      "test/Alphonse de Lamartine\n",
      "Sat Apr  1 19:58:27 2023\n",
      "test/Anatole France\n",
      "Sat Apr  1 19:58:27 2023\n",
      "test/Jules Verne\n",
      "Sat Apr  1 19:58:28 2023\n",
      "test/Victor Hugo\n",
      "Sat Apr  1 19:58:28 2023\n",
      "test/Guy de Maupassant\n",
      "Sat Apr  1 19:58:29 2023\n",
      "test/Alexandre Dumas\n",
      "Sat Apr  1 19:58:29 2023\n",
      "test/Marcel Proust\n",
      "Sat Apr  1 19:58:30 2023\n",
      "test/Gustave Flaubert\n",
      "Sat Apr  1 19:58:31 2023\n",
      "Generating embeddings for embedding camembert-wiki\n",
      "valid1/George Sand\n",
      "Sat Apr  1 19:58:31 2023\n",
      "valid1/Émile Zola\n",
      "Sat Apr  1 19:59:43 2023\n",
      "valid1/Alphonse de Lamartine\n",
      "Sat Apr  1 20:00:46 2023\n",
      "valid1/Anatole France\n",
      "Sat Apr  1 20:01:48 2023\n",
      "valid1/Jules Verne\n",
      "Sat Apr  1 20:02:48 2023\n",
      "valid1/Victor Hugo\n",
      "Sat Apr  1 20:03:51 2023\n",
      "valid1/Guy de Maupassant\n",
      "Sat Apr  1 20:05:03 2023\n",
      "valid1/Alexandre Dumas\n",
      "Sat Apr  1 20:06:03 2023\n",
      "valid1/Marcel Proust\n",
      "Sat Apr  1 20:06:59 2023\n",
      "valid1/Gustave Flaubert\n",
      "Sat Apr  1 20:07:58 2023\n",
      "valid2/George Sand\n",
      "Sat Apr  1 20:08:57 2023\n",
      "valid2/Émile Zola\n",
      "Sat Apr  1 20:09:56 2023\n",
      "valid2/Alphonse de Lamartine\n",
      "Sat Apr  1 20:11:01 2023\n",
      "valid2/Anatole France\n",
      "Sat Apr  1 20:12:07 2023\n",
      "valid2/Jules Verne\n",
      "Sat Apr  1 20:13:12 2023\n",
      "valid2/Victor Hugo\n",
      "Sat Apr  1 20:14:10 2023\n",
      "valid2/Guy de Maupassant\n",
      "Sat Apr  1 20:15:28 2023\n",
      "valid2/Alexandre Dumas\n",
      "Sat Apr  1 20:16:35 2023\n",
      "valid2/Marcel Proust\n",
      "Sat Apr  1 20:17:43 2023\n",
      "valid2/Gustave Flaubert\n",
      "Sat Apr  1 20:18:54 2023\n",
      "valid3/George Sand\n",
      "Sat Apr  1 20:20:03 2023\n",
      "valid3/Émile Zola\n",
      "Sat Apr  1 20:21:05 2023\n",
      "valid3/Alphonse de Lamartine\n",
      "Sat Apr  1 20:22:05 2023\n",
      "valid3/Anatole France\n",
      "Sat Apr  1 20:23:10 2023\n",
      "valid3/Jules Verne\n",
      "Sat Apr  1 20:24:18 2023\n",
      "valid3/Victor Hugo\n",
      "Sat Apr  1 20:25:28 2023\n",
      "valid3/Guy de Maupassant\n",
      "Sat Apr  1 20:26:28 2023\n",
      "valid3/Alexandre Dumas\n",
      "Sat Apr  1 20:27:29 2023\n",
      "valid3/Marcel Proust\n",
      "Sat Apr  1 20:28:29 2023\n",
      "valid3/Gustave Flaubert\n",
      "Sat Apr  1 20:29:40 2023\n",
      "valid4/George Sand\n",
      "Sat Apr  1 20:30:50 2023\n",
      "valid4/Émile Zola\n",
      "Sat Apr  1 20:31:59 2023\n",
      "valid4/Alphonse de Lamartine\n",
      "Sat Apr  1 20:33:05 2023\n",
      "valid4/Anatole France\n",
      "Sat Apr  1 20:34:11 2023\n",
      "valid4/Jules Verne\n",
      "Sat Apr  1 20:35:19 2023\n",
      "valid4/Victor Hugo\n",
      "Sat Apr  1 20:36:26 2023\n",
      "valid4/Guy de Maupassant\n",
      "Sat Apr  1 20:37:38 2023\n",
      "valid4/Alexandre Dumas\n",
      "Sat Apr  1 20:38:50 2023\n",
      "valid4/Marcel Proust\n",
      "Sat Apr  1 20:40:02 2023\n",
      "valid4/Gustave Flaubert\n",
      "Sat Apr  1 20:41:14 2023\n",
      "test/George Sand\n",
      "Sat Apr  1 20:42:28 2023\n",
      "test/Émile Zola\n",
      "Sat Apr  1 20:43:38 2023\n",
      "test/Alphonse de Lamartine\n",
      "Sat Apr  1 20:44:48 2023\n",
      "test/Anatole France\n",
      "Sat Apr  1 20:45:58 2023\n",
      "test/Jules Verne\n",
      "Sat Apr  1 20:47:04 2023\n",
      "test/Victor Hugo\n",
      "Sat Apr  1 20:48:10 2023\n",
      "test/Guy de Maupassant\n",
      "Sat Apr  1 20:49:17 2023\n",
      "test/Alexandre Dumas\n",
      "Sat Apr  1 20:50:27 2023\n",
      "test/Marcel Proust\n",
      "Sat Apr  1 20:51:34 2023\n",
      "test/Gustave Flaubert\n",
      "Sat Apr  1 20:52:42 2023\n",
      "Generating embeddings for embedding camembert-large\n",
      "valid1/George Sand\n",
      "Sat Apr  1 20:53:50 2023\n",
      "valid1/Émile Zola\n",
      "Sat Apr  1 20:57:45 2023\n",
      "valid1/Alphonse de Lamartine\n",
      "Sat Apr  1 21:01:39 2023\n",
      "valid1/Anatole France\n",
      "Sat Apr  1 21:05:24 2023\n",
      "valid1/Jules Verne\n",
      "Sat Apr  1 21:09:02 2023\n",
      "valid1/Victor Hugo\n",
      "Sat Apr  1 21:12:51 2023\n",
      "valid1/Guy de Maupassant\n",
      "Sat Apr  1 21:16:45 2023\n",
      "valid1/Alexandre Dumas\n",
      "Sat Apr  1 21:20:40 2023\n",
      "valid1/Marcel Proust\n",
      "Sat Apr  1 21:24:28 2023\n",
      "valid1/Gustave Flaubert\n",
      "Sat Apr  1 21:28:22 2023\n",
      "valid2/George Sand\n",
      "Sat Apr  1 21:32:12 2023\n",
      "valid2/Émile Zola\n",
      "Sat Apr  1 21:35:57 2023\n",
      "valid2/Alphonse de Lamartine\n",
      "Sat Apr  1 21:39:41 2023\n",
      "valid2/Anatole France\n",
      "Sat Apr  1 21:43:24 2023\n",
      "valid2/Jules Verne\n",
      "Sat Apr  1 21:47:10 2023\n",
      "valid2/Victor Hugo\n",
      "Sat Apr  1 21:50:53 2023\n",
      "valid2/Guy de Maupassant\n",
      "Sat Apr  1 21:55:04 2023\n",
      "valid2/Alexandre Dumas\n",
      "Sat Apr  1 21:58:45 2023\n",
      "valid2/Marcel Proust\n",
      "Sat Apr  1 22:02:26 2023\n",
      "valid2/Gustave Flaubert\n",
      "Sat Apr  1 22:05:40 2023\n",
      "valid3/George Sand\n",
      "Sat Apr  1 22:09:23 2023\n",
      "valid3/Émile Zola\n",
      "Sat Apr  1 22:13:13 2023\n",
      "valid3/Alphonse de Lamartine\n",
      "Sat Apr  1 22:17:00 2023\n",
      "valid3/Anatole France\n",
      "Sat Apr  1 22:20:46 2023\n",
      "valid3/Jules Verne\n",
      "Sat Apr  1 22:24:33 2023\n",
      "valid3/Victor Hugo\n",
      "Sat Apr  1 22:28:17 2023\n",
      "valid3/Guy de Maupassant\n",
      "Sat Apr  1 22:32:04 2023\n",
      "valid3/Alexandre Dumas\n",
      "Sat Apr  1 22:35:54 2023\n",
      "valid3/Marcel Proust\n",
      "Sat Apr  1 22:39:41 2023\n",
      "valid3/Gustave Flaubert\n",
      "Sat Apr  1 22:43:33 2023\n",
      "valid4/George Sand\n",
      "Sat Apr  1 22:47:30 2023\n",
      "valid4/Émile Zola\n",
      "Sat Apr  1 22:51:15 2023\n",
      "valid4/Alphonse de Lamartine\n",
      "Sat Apr  1 22:54:59 2023\n",
      "valid4/Anatole France\n",
      "Sat Apr  1 22:58:38 2023\n",
      "valid4/Jules Verne\n",
      "Sat Apr  1 23:02:22 2023\n",
      "valid4/Victor Hugo\n",
      "Sat Apr  1 23:06:03 2023\n",
      "valid4/Guy de Maupassant\n",
      "Sat Apr  1 23:09:56 2023\n",
      "valid4/Alexandre Dumas\n",
      "Sat Apr  1 23:13:49 2023\n",
      "valid4/Marcel Proust\n",
      "Sat Apr  1 23:17:39 2023\n",
      "valid4/Gustave Flaubert\n",
      "Sat Apr  1 23:21:31 2023\n",
      "test/George Sand\n",
      "Sat Apr  1 23:25:40 2023\n",
      "test/Émile Zola\n",
      "Sat Apr  1 23:29:22 2023\n",
      "test/Alphonse de Lamartine\n",
      "Sat Apr  1 23:33:11 2023\n",
      "test/Anatole France\n",
      "Sat Apr  1 23:36:57 2023\n",
      "test/Jules Verne\n",
      "Sat Apr  1 23:40:36 2023\n",
      "test/Victor Hugo\n",
      "Sat Apr  1 23:44:19 2023\n",
      "test/Guy de Maupassant\n",
      "Sat Apr  1 23:47:55 2023\n",
      "test/Alexandre Dumas\n",
      "Sat Apr  1 23:51:11 2023\n",
      "test/Marcel Proust\n",
      "Sat Apr  1 23:54:28 2023\n",
      "test/Gustave Flaubert\n",
      "Sat Apr  1 23:57:53 2023\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"embeddings\"):\n",
    "    os.mkdir(\"embeddings\")\n",
    "\n",
    "for emb_model, vector_size in zip(emb_models, vector_sizes):\n",
    "    \n",
    "    print(\"Generating embeddings for embedding {}\".format(emb_model))\n",
    "    for t in [\"valid1\", \"valid2\", \"valid3\", \"valid4\", \"test\"]:\n",
    "        generate_embeddings(emb_model, vector_size, t, os.path.join(\"embeddings\", emb_model+'_'+t+'.npz'), overwrite=False) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
